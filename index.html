<!DOCTYPE html>
<html>
<head>
    <title>Umar Iqbal</title>

    <!-- Meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Bootstrap CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom Styles -->
    <style>

        :root {
            --bg: #0a0e13;
            --surface: #0d1117;
            --surface-2: #161b22;
            --surface-3: #1c2128;
            --border: #30363d;
            --border-light: #21262d;
            --text: #e6edf3;
            --text-secondary: #8b949e;
            --accent: #76b900;
            --accent-hover: #8fd400;
            --accent-dim: rgba(118, 185, 0, 0.08);
            --accent-glow: rgba(118, 185, 0, 0.18);
            --red: #f85149;
        }

        * { box-sizing: border-box; }

        html { scroll-behavior: smooth; }

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            font-size: 15px;
            background-color: var(--bg);
            color: var(--text-secondary);
            line-height: 1.75;
            margin: 0;
        }

        h1 {
            font-weight: 700;
            font-size: 1.5rem;
            color: var(--text);
            letter-spacing: -0.02em;
        }

        h2 {
            font-weight: 600;
            font-size: 0.7rem;
            color: var(--accent);
            letter-spacing: 0.12em;
            text-transform: uppercase;
            margin-bottom: 0;
        }

        h4 {
            color: var(--text);
            font-weight: 600;
            font-size: 0.7rem;
            letter-spacing: 0.12em;
            text-transform: uppercase;
        }

        hr {
            border: none;
            border-top: 1px solid var(--border);
            margin: 6px 0 0 0;
        }

        p { color: var(--text-secondary); margin-bottom: 0.75rem; }

        a {
            color: var(--accent);
            text-decoration: none;
            transition: color 0.15s;
        }

        a:hover {
            color: var(--accent-hover);
            text-decoration: none;
        }

        /* ── Navbar ──────────────────────────────── */
        #navbar {
            position: sticky;
            top: 0;
            z-index: 1000;
            background: rgba(10, 14, 19, 0.9);
            backdrop-filter: blur(14px);
            -webkit-backdrop-filter: blur(14px);
            border-bottom: 1px solid var(--border-light);
        }

        #navbar .container {
            display: flex;
            align-items: center;
            justify-content: space-between;
            height: 52px;
            max-width: 960px;
        }

        .nav-brand {
            font-weight: 600;
            font-size: 13px;
            color: var(--text);
            letter-spacing: 0.03em;
        }

        .nav-links {
            display: flex;
            gap: 28px;
            list-style: none;
            margin: 0;
            padding: 0;
        }

        .nav-links a {
            color: var(--text-secondary);
            font-size: 12px;
            font-weight: 500;
            letter-spacing: 0.08em;
            text-transform: uppercase;
            padding: 4px 0;
            border-bottom: 2px solid transparent;
            transition: color 0.15s, border-color 0.15s;
        }

        .nav-links a:hover,
        .nav-links a.active {
            color: var(--accent);
            border-bottom-color: var(--accent);
        }

        /* ── Header ──────────────────────────────── */
        #header {
            background-color: var(--surface);
            border-bottom: 1px solid var(--border-light);
            display: flex;
            align-items: flex-end;
            padding-top: 64px;
            padding-bottom: 64px;
        }

        #header .container { max-width: 960px; }

        #portrait {
            border: 3px solid var(--border);
            border-radius: 50%;
            display: block;
            transition: border-color 0.3s, box-shadow 0.3s;
        }

        #portrait:hover {
            border-color: var(--accent);
            box-shadow: 0 0 24px var(--accent-glow);
        }

        #header-text { margin-top: 0; margin-left: 0; }

        #header-text-name {
            font-size: 36px;
            font-weight: 700;
            color: var(--text);
            letter-spacing: -0.03em;
            line-height: 1.15;
            margin-bottom: 6px;
        }

        #header-text-title {
            font-size: 14px;
            color: var(--text-secondary);
        }

        #header-text-title a { color: var(--accent); }

        #header-text-affiliation {
            font-size: 14px;
            color: var(--text-secondary);
            margin-top: 6px;
        }

        #header-text-email {
            margin-top: 8px;
            font-size: 14px;
            font-style: italic;
        }

        .header-text-desc { font-size: 17px; }

        .nav-buttons {
            margin-top: 18px;
            display: flex;
            gap: 8px;
            flex-wrap: wrap;
        }

        .nav-buttons a {
            display: inline-block;
            padding: 4px 14px;
            border: 1px solid var(--border);
            border-radius: 6px;
            font-size: 12px;
            font-weight: 500;
            color: var(--text-secondary);
            letter-spacing: 0.02em;
            transition: border-color 0.15s, color 0.15s, background 0.15s;
        }

        .nav-buttons a:hover {
            border-color: var(--accent);
            color: var(--accent);
            background: var(--accent-dim);
        }

        /* ── Footer ──────────────────────────────── */
        #footer {
            background-color: var(--surface);
            padding: 40px;
            border-top: 1px solid var(--border-light);
            text-align: center;
            font-size: 12px;
            color: var(--text-secondary);
        }

        /* ── Spacing ─────────────────────────────── */
        .vspace-top { margin-top: 36px; }
        .vspace-top-news { margin-top: 10px; }

        /* ── Main container ──────────────────────── */
        .container { max-width: 960px; }

        /* ── Bio / About ─────────────────────────── */
        p.justified { text-align: justify; }

        /* ── News ────────────────────────────────── */
        .news-date { font-weight: 600; color: var(--text); }

        .date {
            color: var(--red);
            display: inline-block;
            width: 90px;
            font-size: 12px;
            font-weight: 500;
            font-variant-numeric: tabular-nums;
        }

        .entry p {
            margin: 5px 0;
            padding: 0;
            font-size: 13.5px;
            line-height: 1.6;
        }

        /* ── Publication rows ────────────────────── */
        .row {
            display: flex;
            align-items: center;
        }

        .col-sm-3 {
            display: flex;
            align-items: center;
            justify-content: center;
        }

        .col-sm-3 img,
        .col-sm-3 video {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            display: block;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }

        .col { display: flex; flex-direction: column; justify-content: center; }

        /* Paper card hover */
        .row.vspace-top:has(.paper-col) {
            padding: 14px 10px;
            border-radius: 8px;
            border: 1px solid transparent;
            margin-left: -10px;
            margin-right: -10px;
            transition: background 0.2s, border-color 0.2s;
        }

        .row.vspace-top:has(.paper-col):hover {
            background: var(--surface-2);
            border-color: var(--border);
        }

        .row.vspace-top:has(.paper-col):hover .col-sm-3 img,
        .row.vspace-top:has(.paper-col):hover .col-sm-3 video {
            transform: scale(1.03);
            box-shadow: 0 4px 22px var(--accent-glow);
        }

        /* Paper text */
        .paper-title {
            font-weight: 600;
            color: var(--text);
            font-size: 14.5px;
            line-height: 1.45;
        }

        .paper-desc { }

        .paper-authors {
            font-style: normal;
            color: var(--text-secondary);
            font-size: 13px;
            margin-top: 4px;
            line-height: 1.55;
        }

        .paper-authors b { color: var(--accent); }

        .paper-authors u {
            color: var(--text);
            text-decoration: none;
            font-weight: 600;
        }

        /* Paper links */
        .col-sm-9 > div:not([class]) a,
        .col-sm-9 > div:not([class]) {
            margin-top: 8px;
        }

        .col-sm-9 > div:not([class]) a {
            display: inline-block;
            padding: 2px 10px;
            margin-right: 4px;
            border: 1px solid var(--border);
            border-radius: 4px;
            font-size: 12px;
            color: var(--text-secondary);
            transition: border-color 0.15s, color 0.15s, background 0.15s;
        }

        .col-sm-9 > div:not([class]) a:hover {
            border-color: var(--accent);
            color: var(--accent);
            background: var(--accent-dim);
        }

        /* ── Scroll fade-in ──────────────────────── */
        .fade-in {
            opacity: 0;
            transform: translateY(18px);
            transition: opacity 0.5s ease, transform 0.5s ease;
        }

        .fade-in.visible {
            opacity: 1;
            transform: translateY(0);
        }

        /* ── Mobile ──────────────────────────────── */
        @media (max-width: 767px) {
            #navbar .container {
                padding: 0 16px;
            }

            .nav-links { gap: 18px; }
            .nav-links a { font-size: 11px; }

            #header {
                padding-top: 40px;
                padding-bottom: 40px;
            }

            #header .row {
                flex-direction: column;
                align-items: center;
                text-align: center;
            }

            #header .col-sm-3 {
                width: 110px !important;
                max-width: 110px;
                margin: 0 auto 20px auto;
                flex: none;
            }

            #header-text-name { font-size: 28px; }

            .nav-buttons { justify-content: center; }

            .row.vspace-top:has(.paper-col) {
                flex-direction: column;
                align-items: flex-start;
                margin-left: 0;
                margin-right: 0;
                padding: 12px 8px;
            }

            .col-sm-3 {
                width: 100% !important;
                max-width: 180px;
                margin: 0 auto 12px auto;
                flex: none;
            }

            .col-sm-9 {
                width: 100% !important;
                flex: none;
            }
        }

        @media (max-width: 480px) {
            .nav-brand { display: none; }
            .nav-links { gap: 14px; }
        }

    </style>
</head>

<body>

    <!-- Sticky Navbar -->
    <nav id="navbar">
        <div class="container">
            <span class="nav-brand">Umar Iqbal</span>
            <ul class="nav-links">
                <li><a href="#about">About</a></li>
                <li><a href="#news">News</a></li>
                <li><a href="#publications">Publications</a></li>
            </ul>
        </div>
    </nav>

    <div id='header'>
        <div class='container'>
            <div class='row'>
                <div class="col-sm-3 offset-sm-1">
                    <img src='imgs/portrait.jpeg' class='img-fluid' id='portrait'>
                </div>

                <div class="col">
                  <div id='header-text-name'>
                      Umar Iqbal
                  </div>
                    <div id='header-text-title'>
                        Senior Research Manager, <a href="https://research.nvidia.com/labs/dair/">Data-Drive AI for Robotics (DAIR)</a>, NVIDIA
                    </div>
                    
                  
                  <div class="nav-buttons">
                    <a href="https://scholar.google.de/citations?user=QSKXFiYAAAAJ&hl=en">Scholar</a>
                    <a href="https://www.linkedin.com/in/iqbalu/">LinkedIn</a>
                    <a href="assets/resume.pdf">Resume</a>
                  </div>

                </div>
            </div>
        </div>
    </div>


    <div class='container vspace-top' id="about" style="scroll-margin-top: 68px;">
                <p class="justified">
                    I am a Sr. Research Manager at <a href="https://www.nvidia.com/en-us/research/">NVIDIA Research</a>, 
                    leading the <a href="https://research.nvidia.com/labs/dair/">Data-Drive AI for Robotics (DAIR)</a> team. 
                    The team investigates how robots can learn directly from human data, such as videos, motion capture, 
                    and large-scale demonstrations, to acquire skills that generalize across tasks, embodiments, and environments. 
                    We work at the intersection of computer vision, machine learning, and robotics, developing models that 
                    understand, reconstruct, and imitate human behaviors. 
                    I earned my PhD in Computer Science (2014-2018) from the <a href="https://www.uni-bonn.de/en">University of Bonn</a>, Germany, under the 
                    guidance of Prof. <a href="https://pages.iai.uni-bonn.de/gall_juergen/">Juergen Gall</a>. 
                    Prior to that, I completed my masters (2011-2013) in Finland and undergrad (2006-2010) in Pakistan.  
                    I developed my passion for computer vision and machine learning in 2009 during my undergrad thesis on vehicle make and model recognition 
                    and have been in love with the field ever since.
                </p>
    
            <!-- 
             <p><strong style="color: red; text-decoration: line-through;">I am hiring multiple Full-time Research Scientists to join the team. <a href="https://nvidia.wd5.myworkdayjobs.com/en-US/NVIDIAExternalCareerSite/job/Research-Scientist--Digital-Humans---New-College-Grad-2025_JR1990211">Apply here.</a> </strong></p> 
            -->
             <p><strong>We are always looking for motivated research interns. Feel free to <a href="mailto:uiqbal@nvidia.com">reach out</a> if you are interested.</strong></p>
    </div><!-- /#about -->

    <div style="height: 36px;"></div>



    <div class="container">
    <div id="news" style="scroll-margin-top: 68px;"></div>
    <h4>News!</h4>
        <div class="entry">
        <p><span class="date">25/06/2025:</span>Four papers accepted to ICCV 2025 including <a href="https://research.nvidia.com/labs/dair/genmo/">GENMO</a>, <a href="https://research.nvidia.com/labs/dair/geoman/">GeoMan</a>, <a href="https://nvlabs.github.io/AdaHuman/">AdaHuman</a>, and HumanOLAT.
        <p><span class="date">05/05/2025:</span><a href="https://research.nvidia.com/labs/dair/genmo/">GENMO</a> is available on arXiv. Generate human motions from multiple modalities including text, audio, and video.
        <p><span class="date">12/12/2024:</span><a href="https://research.nvidia.com/labs/dair/simavatar/">SimAvatar</a> is accepted to CVPR 2025. Generate SimReady avatars with just text prompts.
        <p><span class="date">22/01/2024:</span>A paper on <a href="https://mathis.petrovich.fr/stmc/">text-driven 3D human motion generation</a> is now available on arXiv. 
        <p><span class="date">05/01/2024:</span><a href="https://research.nvidia.com/labs/nxp/wysiwyg/">What You See is What You GAN</a>, now available on arXiv. 
        <p><span class="date">18/12/2023:</span><a href="https://nvlabs.github.io/GAvatar/">GAvatar</a> is available on arXiv now. Make Gaussian avatars using simple text descriptions.</p>
        </div>
    <!-- 
        <p><span class="date">21/10/2023:</span><a href="https://nvlabs.github.io/PACE/">PACE</a> accepted to 3DV'24.</p>
        <p><span class="date">11/12/2023:</span>One paper accepted to NeurIPs'23.</p>
        <p><span class="date">20/07/2023:</span>Three papers accepted to ICCV'23.</p>
        <p><span class="date">24/02/2022:</span>Two papers accepted to CVPR'22.</p>
        <p><span class="date">10/07/2021:</span>I am serving as an Area Chair for <a href="https://www.bmvc2021.com/">BMVC'21</a>.</p>
        <p><span class="date">24/02/2020:</span>Two papers accepted to <a href="https://cvpr2020.thecvf.com/">CVPR'20</a>. One on weakly-supervised 3D human pose estimation and the other on self-supervised viewpoint learning. More info to follow.</p>
        <p><span class="date">22/07/2019:</span>A paper on <a href="https://arxiv.org/abs/1905.01941">few-shot adaptive gaze estimation</a> has been accepted to <a href="https://iccv2019.thecvf.com/">ICCV'19</a>. Code coming soon!</p>
        <p><span class="date">05/07/2019:</span>My dissertation has won the DAGM MVTec Dissertation Award 2019. DAGM (German Association for Pattern Recognition) represents the German community for pattern recognition including computer vision and machine learning.</p>
        <p><span class="date">14/01/2019:</span>I have joined <a href="https://jankautz.com/">Jan Kautz's</a> lab at <a href="https://www.nvidia.com/en-us/research/">Nvidia Research</a> as a post-doctoral Research Scientist.</p>
        <p><span class="date">30/11/2018:</span><b>Successfully defended my PhD</b> with the highest possible distinction <i>(summa cum laude)</i>.</p>
        <p><span class="date">09/09/2018:</span>Received the <b>Best Poster Award</b> for our work on <a href="https://arxiv.org/pdf/1804.09534.pdf">hand pose estimation</a> at the <a href="https://sites.google.com/view/hands2018/home">4th International Workshop on Observing and Understanding Hands in Actions (HANDS),</a> 2018.</p>
        <p><span class="date">03/07/2018:</span>Our work on 2D and 3D <a href="https://arxiv.org/pdf/1804.09534.pdf">hand pose estimation</a> is accepted to ECCV 2018.</p>
        <p><span class="date">03/07/2018:</span><a href="https://arxiv.org/abs/1805.04596">JointFlow</a> paper on multi-person pose tracking is accepted to <a href="http://bmvc2018.org/">BMVC-2018</a>.</p>
        <p><span class="date">12/05/2018:</span>JointFlow paper on multi-person pose tracking is available online.</p>
        <p><span class="date">25/04/2018:</span>A paper on 2D and 3D hand pose estimation from RGB images is available on <a href="https://arxiv.org/pdf/1804.09534.pdf">ArXiv.</a></p>
        <p><span class="date">30/03/2018:</span>We are organizing <b>2nd PoseTrack Challenge and Workshop</b> in conjunction with <a href="https://eccv2018.org/">ECCV-2018.</a> in Munich. Stay tuned at <a href="http://www.posetrack.net/">http://www.posetrack.net</a> for more info!</p>
        <p><span class="date">30/09/2017:</span>I will spend next six months as Research Intern in <a href="https://jankautz.com/">Jan Kautz's</a> lab at <a href="https://www.nvidia.com/en-us/research/">Nvidia Research</a>.</p>
        <p><span class="date">26/09/2017:</span>Evaluation server for <a href="http://www.posetrack.net/">PoseTrack</a> challenge is now open. All the best to all participants.</p>
        <p><span class="date">10/07/2017:</span>Pre-release of <a href="http://www.posetrack.net/">PoseTrack</a> dataset is available now. Grab your copy <a href="https://posetrack.net/">here</a>.</p>
        <p><span class="date">15/06/2017:</span><a href="http://posetrack.net/workshops/iccv2017">Webpage</a> for <a href="http://posetrack.net/workshops/iccv2017">PoseTrack Challenge</a> is down as we are moving our website to a new server. Apologies for the downtime. Will be accessible again soon.</p>
        <p><span class="date">08/05/2017:</span>The extended version of our <a href="https://arxiv.org/abs/1509.06720">CVPR-2016</a> paper on 3D human pose estimation is <a href="http://www.umariqbal.info/uploads/1/4/8/3/14837880/tpami2017_3dpose.pdf">available</a>.<a href="https://arxiv.org/abs/1705.02883">[arXiv]</a>.</p>
        <p><span class="date">05/04/2017:</span><a href="http://posetrack.net/workshops/iccv2017">Webpage</a> for <a href="http://posetrack.net/workshops/iccv2017">PoseTrack Challenge</a> is up!</p>
        <p><span class="date">31/03/2017:</span>We are organizing the <b>PoseTrack Challenge</b> on Human Pose Estimation and Tracking in the Wild at <a href="http://iccv2017.thecvf.com/">ICCV-2017</a>. Stay tuned at <a href="http://www.posetrack.net/">http://www.posetrack.net</a> for more info!</p>
        <p><span class="date">26/02/2017:</span>Our paper on <a href="http://www.umariqbal.info/publications.html">Multi-Person Pose Estimation and Tracking</a> accepted to <a href="https://cvpr2017.thecvf.com/">CVPR-2017</a>, <a href="https://arxiv.org/abs/1705.02883">arXiv</a>. <br><span style="color: red">Source code, models, and dataset coming Soon...<span></p>
        <p><span class="date">23/01/2017:</span>Our paper on <a href="http://www.umariqbal.info/publications.html">action conditioned human pose estimation</a> accepted to <a href="http://www.fg2017.org/">FG-2017</a>, <a href="https://arxiv.org/abs/1705.02883">arXiv</a> Project Page <span style="color: red">(Coming Soon)<span>.</p>
        <p><span class="date">29/11/2016:</span>A paper on multi-person pose tracking is available on <a href="https://arxiv.org/abs/1705.02883">arXiv</a>. <a href="http://pages.iai.uni-bonn.de/iqbal_umar/pose-track/">Project Page</a>.</p>
        <p><span class="date">24/08/2016:</span>A paper on multi-person human pose estimation accepted to <a href="http://www.eccv2016.org/">ECCV</a>, Workshop on <a href="http://www.crowd-understanding.eu/">Crowd Understanding</a>,2016. <a href="http://pages.iai.uni-bonn.de/iqbal_umar/pose-track/">Project Page</a>.</p>
        <p><span class="date">14/03/2016:</span>A paper on <a href="http://www.umariqbal.info/publications.html">action conditioned human pose estimation</a> is available on <a href="https://arxiv.org/abs/1705.02883">arXiv</a>.</p>
        <p><span class="date">02/03/2016:</span>A paper on <a href="http://www.umariqbal.info/publications--projects.html">3D human pose estimation</a> accepted to <a href="https://cvpr2016.thecvf.com/">CVPR 2016</a>! <a href="http://pages.iai.uni-bonn.de/iqbal_umar/pose-track/">Project Page</a> is up.</p>
        <p><span class="date">05/02/2014:</span>Starting doctoral studies at the <a href="https://www3.uni-bonn.de/informatik-cv/">Computer Vision Research Group (Prof. Dr. Juergen Gall)</a>, <a href="https://www.uni-bonn.de/en">University of Bonn, </a>Germany.</p>
    </div> -->

       
    <div class='vspace-top' id="publications" style="scroll-margin-top: 68px;">
        <h1>Publications</h1>
    </div>

    <div class='vspace-top'>
        <h2>2026</h2>
        <hr> 
    </div>

    <div class='row vspace-top'>

        <div class="col-sm-3 center-align">            
            <video class="video" width=80% height=auto muted autoplay loop playsinline style="display: block; margin: 0 auto;">
                <source src="imgs/2026/egocontrol.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>            
        </div>

        <div class="col-sm-9 paper-col">
            <div class='paper-title'>
                EgoControl: Controllable Egocentric Video Generation via 3D Full-Body Poses
            </div>

            <div class='paper-desc'>
            </div>
            <div class='paper-authors'>
                Enrico Pallotta, Sina Mokhtarzadeh Azar, Lars Doorenbos, Serdar Ozsoy, <u>Umar Iqbal</u>, Juergen Gall
                <br><b>CVPR 2026</b>
            </div>
            <div>
                <a href="https://arxiv.org/abs/2511.18173">[PDF]</a>
                <a href="https://pallottaenrico.github.io/ego-cosmos/">[Project Page]</a>
            </div>
        </div>
    </div>



    <div class='row vspace-top'>

        <div class="col-sm-3 center-align">            
            <video class="video" width=80% height=auto muted autoplay loop playsinline style="display: block; margin: 0 auto;">
                <source src="imgs/2026/sonic.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>            
        </div>

        <div class="col-sm-9 paper-col">
            <div class='paper-title'>
                SONIC: Supersizing Motion Tracking for Natural Humanoid Whole-Body Control
            </div>

            <div class='paper-desc'>
            </div>
            <div class='paper-authors'>
                Zhengyi Luo, Ye Yuan, Tingwu Wang, Chenran Li, Sirui Chen, Fernando Castañeda, Zi-Ang Cao, Jiefeng Li, David Minor, Qingwei Ben, Xingye Da, Runyu Ding, Cyrus Hogg, Lina Song, Edy Lim, Eugene Jeong, Tairan He, Haoru Xue, Wenli Xiao, Zi Wang, Simon Yuen, Jan Kautz, Yan Chang, <u>Umar Iqbal</u>, Linxi "Jim" Fan, Yuke Zhu
                <br><b>arXiv 2025</b></u>
            </div>
            <div>
                <a href="https://arxiv.org/abs/2511.07820">[PDF]</a>
            </div>
        </div>
    </div>



    <div class='row vspace-top'>

        <div class="col-sm-3 center-align">            
            <video class="video" width=80% height=auto muted autoplay loop playsinline style="display: block; margin: 0 auto;">
                <source src="imgs/2026/dla.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>            
        </div>

        <div class="col-sm-9 paper-col">
            <div class='paper-title'>
                Dream, Lift, Animate: From Single Images to Animatable Gaussian Avatars
            </div>

            <div class='paper-desc'>
            </div>
            <div class='paper-authors'>
                Marcel C. Buehler, Ye Yuan, Xueting Li, Yangyi Huang, Koki Nagano, <u>Umar Iqbal</u>
                <br><b>3DV 2026</b></u>
            </div>
            <div>
                <a href="https://arxiv.org/abs/2507.15979">[PDF]</a>
                <a href="https://research.nvidia.com/labs/dair/dream-lift-animate/">[Project Page]</a>
            </div>
        </div>
    </div>




    <div class='vspace-top'>
        <h2>2025</h2>
        <hr> 
    </div>

    <div class='row vspace-top'>
        <div class="col-sm-3 center-align">
            <img src="imgs/2025/genmo.png" class="img-fluid" style="max-width: 50%; display: block; margin: 0 auto;">
        </div>

        <div class="col-sm-9 paper-col">
            <div class='paper-title'>
                GENMO: A <u>GEN</u>arlist Model for Human <u>MO</u>tion 
            </div>

            <div class='paper-desc'>
            </div>
            <div class='paper-authors'>
                Jiefeng Li, Jinkun Cao, Haotian Zhang, Davis Rempe, Jan Kautz, <u>Umar Iqbal</u>, Ye Yuan 
                <br><b>ICCV 2025</b></u>
            </div>
            <div>
                <a href="https://arxiv.org/abs/2505.01425">[PDF]</a>
                <a href="https://research.nvidia.com/labs/dair/genmo/">[Project Page]</a>
                <a href="https://www.youtube.com/watch?v=b2lCM3kLZPI">[Video]</a>
            </div>
        </div>
    </div>

    <div class='row vspace-top'>
        <div class="col-sm-3 center-align">            
            <video class="video" width=80% height=auto muted autoplay loop playsinline style="display: block; margin: 0 auto;">
                <source src="imgs/2025/geoman.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>            
        </div>

        <div class="col-sm-9 paper-col">
            <div class='paper-title'>
                GeoMan: Temporally Consistent Human Geometry Estimation using Image-to-Video Diffusion
            </div>

            <div class='paper-desc'>
            </div>
            <div class='paper-authors'>
                Gwanghyun Kim, Xueting Li, Ye Yuan, Koki Nagano, Tianye Li, Jan Kautz, Se Young Chun, <u>Umar Iqbal</u>
                <br><b>ICCV 2025</b></u>
            </div>
            <div>
                <a href="https://arxiv.org/abs/2505.23085">[PDF]</a>
                <a href="https://research.nvidia.com/labs/dair/geoman/">[Project Page]</a>
            </div>
        </div>
    </div>

    <div class='row vspace-top'>
        <div class="col-sm-3 center-align">            
            <video class="video" width=80% height=auto muted autoplay loop playsinline style="display: block; margin: 0 auto;">
                <source src="imgs/2025/adahuman.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>            
        </div>


        <div class="col-sm-9 paper-col">
            <div class='paper-title'>
                AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion            </div>
            <div class='paper-desc'>
            </div>
            <div class='paper-authors'>
                Yangyi Huang, Ye Yuan, Xueting Li, Jan Kautz, <u>Umar Iqbal</u>
                <br><b>ICCV 2025</b></u>
            </div>
            <div>
                <a href="https://arxiv.org/abs/2505.24877">[PDF]</a>
                <a href="https://nvlabs.github.io/AdaHuman/">[Project Page]</a>
            </div>
        </div>
    </div>

    <div class='row vspace-top'>
        <div class="col-sm-3 center-align">
            <img src="imgs/2025/humanolat.png" class="img-fluid" style="max-width: 75%; display: block; margin: 0 auto;">
        </div>

        <div class="col-sm-9 paper-col">
            <div class='paper-title'>
                HumanOLAT: A Large-Scale Dataset for Full-Body Human Relighting and Novel-View Synthesis
            </div>

            <div class='paper-desc'>
            </div>
            <div class='paper-authors'>
                Timo Teufel, Xilong Zhou, <u>Umar Iqbal</u>, Pramod Rao, Pulkit Gera, Jan Kautz, Vladislav Golyanik, Christian Theobalt
                <br><b>ICCV 2025</b></u>
            </div>
            <div>
                <a href="https://vcai.mpi-inf.mpg.de/projects/HumanOLAT/">[PDF]</a>
                <a href="https://vcai.mpi-inf.mpg.de/projects/HumanOLAT/">[Project Page]</a>
            </div>
        </div>
    </div>




    <div class='row vspace-top'>
        <div class="col-sm-3 center-align">
            <video class="video" width=100% height=auto muted autoplay loop playsinline>
                <source src="imgs/2025/simavatar.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>            
        </div>

        <div class="col-sm-9 paper-col">
            <div class='paper-title'>
                SimAvatar: Simulation-Ready Avatars with Layered Hair and Clothing
            </div>
            <div class='paper-desc'>
            </div>
            <div class='paper-authors'>
                Xueting Li, Ye Yuan, Shalini De Mello, Gilles Daviet, Jonathan Leaf, Miles Macklin, Jan Kautz, <u>Umar Iqbal</u>
                <br><b>CVPR 2025</b></u>
            </div>
            <div>
                <a href="https://nvlabs.github.io/SimAvatar/static/paper.pdf">[PDF]</a>
                <a href="https://nvlabs.github.io/SimAvatar">[Project Page]</a>
                <a href="https://www.youtube.com/watch?v=qEwBY7LBW2Y">[Video]</a>
            </div>
        </div>
    </div>

     <div class='row vspace-top'>
        <div class="col-sm-3 center-align">
            <div style="text-align: center;">
                <video class="video" width=80% height=auto muted autoplay loop playsinline>
                    <source src="imgs/2025/videopanda.mp4" type="video/mp4">
                </video>
            </div>
        </div>

        <div class="col-sm-9 paper-col">
            <div class='paper-title'>
                VideoPanda: Panoramic Video Diffusion with Multi-view Attention
            </div>
            <div class='paper-desc'>
            </div>
            <div class='paper-authors'>
                Kevin Xie*, Amirmojtaba Sabour*, Jiahui Huang, Despoina Paschalidou, Greg Klar, <u>Umar Iqbal</u>, Sanja Fidler, Xiaohui Zeng
                <br><b>arXiv 2025</b></u>
            </div>
            <div>
                <a href="https://arxiv.org/abs/2504.11389">[PDF]</a>
                <a href="https://research.nvidia.com/labs/toronto-ai/VideoPanda/">[Project Page]</a>
            </div>
        </div>
    </div>

   
 
    <!-- 
    <div class='row vspace-top'>
        <div class="col-sm-3 center-align">
            <video class="video" width=100% height=auto muted autoplay loop playsinline>
                <source src="imgs/2025/adahuman.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>            
        </div>

        <div class="col-sm-9 paper-col">
            <div class='paper-title'>
                AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion
            </div>
            <div class='paper-desc'>
            </div>
            <div class='paper-authors'>
                Yangyi Huang, Ye Yuan, Xueting Li, Jan Kautz, <u>Umar Iqbal</u>
                <br><b>arXiv 2024</b></u>
            </div>
            <div>
                <a href="">[PDF]</a>
                <a href="https://nvlabs.github.io/AdaHuman">[Project Page]</a>
                <a href="">[Video]</a>
            </div>
        </div>
    </div>
    --> 

    <div class='vspace-top'>
        <h2>2024</h2>
        <hr> 
    </div>

    <div class='row vspace-top'>
        <div class="col-sm-3 center-align">
            <video class="video" width=100% height=auto muted autoplay loop playsinline>
                <source src="imgs/2024/coin.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>            
        </div>

        <div class="col-sm-9 paper-col">
            <div class='paper-title'>
                COIN: Control-Inpainting Diffusion Prior for Human and Camera Motion Estimation
            </div>
            <div class='paper-desc'>
            </div>
            <div class='paper-authors'>
                Jiefeng Li, Ye Yuan, Davis Rempe, Haotian Zhang, Pavlo Molchanov, Cewu Lu, Jan Kautz, <u> Umar Iqbal</u>
                <br><b>ECCV 2024</b></u>
            </div>
            <div>
                <a href="">[PDF]</a>
                <a href="https://nvlabs.github.io/COIN">[Project Page]</a>
                <a href="">[Video]</a>
            </div>
        </div>
    </div>
  
    <div class='row vspace-top'>
        <div class="col-sm-3 center-align">
            <video class="video" width=100% height=auto muted autoplay loop playsinline>
                <source src="imgs/2024/stmc.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>            
        </div>

        <div class="col-sm-9 paper-col">
            <div class='paper-title'>
                Multi-Track Timeline Control for Text-Driven 3D Human Motion Generation
            </div>
            <div class='paper-desc'>
            </div>
            <div class='paper-authors'>
                Mathis Petrovich, Or Litany, <u> Umar Iqbal</u>, Michael J. Black, Gul Varol, Xue Bin Peng, Davis Rempe
                <br><b>CVPR Workshop on Human Motion Generation 2024</b></u>
            </div>
            <div>
                <a href="https://arxiv.org/abs/2401.08559">[PDF]</a>
                <a href="https://mathis.petrovich.fr/stmc/">[Project Page]</a>
                <a href="https://www.youtube.com/watch?v=_UXhDp0o4HI">[Video]</a>
            </div>
        </div>

    </div>

    <div class='row vspace-top'>
        <div class="col-sm-3 center-align">
            <video class="video" width=100% height=auto muted autoplay loop playsinline>
                <source src="imgs/2024/wyswyg.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>            
        </div>

        <div class="col-sm-9 paper-col">
            <div class='paper-title'>
                What You See is What You GAN: Rendering Every Pixel for High-Fidelity Geometry in 3D GANs
            </div>
            <div class='paper-desc'>
            </div>
            <div class='paper-authors'>
                Alex Trevithick, Matthew Chan, Towaki Takikawa, <u> Umar Iqbal</u>, Shalini De Mello, Manmohan Chandraker, Ravi Ramamoorthi, Koki Nagano 
                <br><b>CVPR 2024</b></u>
            </div>
            <div>
                <a href="https://arxiv.org/abs/2401.02411">[PDF]</a>
                <a href="https://research.nvidia.com/labs/nxp/wysiwyg/">[Project Page]</a>
                <a href="https://www.youtube.com/watch?v=rA3PreBTIac">[Video]</a>
            </div>
        </div>
    </div>


    <div class='row vspace-top'>
        <div class="col-sm-3 center-align">
                <img src='imgs/2024/gavatar.gif' class='img-fluid'>
        </div>

        <div class="col-sm-9 paper-col">
            <div class='paper-title'>
                GAvatar: Animatable 3D Gaussian Avatars with Implicit Mesh Learning
            </div>
            <div class='paper-desc'>
            </div>
            <div class='paper-authors'>
               Ye Yuan, Xueting Li, Yangyi Huang, Shalini De Mello, Koki Nagano, Jan Kautz, <u> Umar Iqbal</u> <br><b>CVPR 2024</b></u>
            </div>
            <div>
                <a href="https://arxiv.org/abs/2312.11461">[PDF]</a>
                <a href="https://nvlabs.github.io/GAvatar/">[Project Page]</a>
                <a href="https://youtu.be/PbCF1HzrKrs?si=CNZMzmeoHYdrA-JM">[Video]</a>
            </div>
        </div>
    </div>

    <div class='row vspace-top'>
        <div class="col-sm-3 center-align">
                <img src='imgs/2024/pace.gif' class='img-fluid'>
        </div>

        <div class="col-sm-9 paper-col">
            <div class='paper-title'>
                PACE: Human and Camera Motion Estimation from in-the-wild Videos
            </div>
            <div class='paper-desc'>
            </div>
            <div class='paper-authors'>
               Muhammed Kocabas, Ye Yuan, Pavlo Molchanov, Yunrong Guo, Michael J. Black, Otmar Hilliges, Jan Kautz, 
                <u> Umar Iqbal</u> <br><b>3DV 2024</b></u>
            </div>
            <div>
                <a href="https://arxiv.org/pdf/2310.13768v1.pdf">[PDF]</a>
                <a href="https://nvlabs.github.io/PACE/">[Project Page]</a>
                <a href="https://www.youtube.com/watch?v=TmU3uttNTF4">[Video]</a>
            </div>
        </div>
    </div>

    <div class='vspace-top'>
        <h2>2023</h2>
        <hr> 
    </div>

    <div class='row vspace-top'>
        <div class="col-sm-3 center-align">
                <img src='imgs/2023/one-shot-neurips-v2.gif' class='img-fluid'>
        </div>

        <div class="col-sm-9 paper-col">
            <div class='paper-title'>
                Generalizable One-shot Neural Head Avatar
            </div>
            <div class='paper-desc'>
            </div>
            <div class='paper-authors'>
                Xueting Li, Shalini De Mello, Sifei Liu, Koki Nagano,<u> Umar Iqbal</u>, Jan Kautz <br><b>NeurIPs 2023</b></u>
            </div>
            <div>
                <a href="https://arxiv.org/pdf/2306.08768.pdf">[PDF]</a>
                <a href="https://research.nvidia.com/labs/lpr/one-shot-avatar/">[Project Page]</a>
                <a href="https://research.nvidia.com/labs/lpr/one-shot-avatar/static/videos/introduction.mp4">[Video]</a>
            </div>
        </div>
    </div>

    <div class='row vspace-top'>
        <div class="col-sm-3 center-align">
                <img src='imgs/2023/rana_3.gif' class='img-fluid'>
        </div>

        <div class="col">
            <div class='paper-title'>
                RANA: Relightable Articulated Neural Avatars 
            </div>
            <div class='paper-desc'>
            </div>
            <div class='paper-authors'>
                <u>Umar Iqbal</u>, Akin Caliskan, Koki Nagano, Sameh Khamis, Pavlo Molchanov, Jan Kautz <br><b>ICCV,  2023</b>
            </div>
            <div>
                <a href="https://arxiv.org/abs/2212.03237/">[PDF]</a>
                <a href="https://nvlabs.github.io/RANA/">[Project Page]</a>
                <a href="https://www.youtube.com/watch?v=s-hIhIMjPqQ">[Video]</a>
            </div>
        </div>
    </div>


    <div class='row vspace-top'>
        <div class="col-sm-3 center-align">
                <img src='imgs/2023/physdiff.gif' class='img-fluid'>
        </div>

        <div class="col">
            <div class='paper-title'>
                PhysDiff: Physics-Guided Human Motion Diffusion Model
            </div>
            <div class='paper-desc'>
                
            </div>
            <div class='paper-authors'>
                Ye Yuan, Jiaming Song,<u> ​Umar Iqbal</u>, Arash Vahdat, Jan Kautz <br><b>ICCV,  2023</b>
            </div>
            <div>
                <a href="https://arxiv.org/abs/2212.02500/">[PDF]</a>
                <a href="https://arxiv.org/abs/2212.02500">[Project Page]</a>
                
            </div>
        </div>
    </div>


    <div class='row vspace-top'>
        <div class="col-sm-3 center-align">
                <img src='imgs/2023/humandynamics-iccv23_1.gif' class='img-fluid'>
        </div>

        <div class="col">
            <div class='paper-title'>
                Learning Human Dynamics in Autonomous Driving Scenarios
            </div>
            <div class='paper-desc'>
                
            </div>
            <div class='paper-authors'>
                Jingbo Wang, Ye Yuan, Zhengyi Luo, Kevin Xie, Dahua Lin, <u>Umar Iqbal</u>, Sanja Fidler, Sameh Khamis<br><b> ICCV,  2023</b></u>
            </div>
            <div>
                <a href="https://openaccess.thecvf.com//content/ICCV2023/papers/Wang_Learning_Human_Dynamics_in_Autonomous_Driving_Scenarios_ICCV_2023_paper.pdf">[PDF]</a>
                <a href="https://www.youtube.com/watch?v=QTQtIDQVJ1I">[Video]</a>
            </div>
        </div>
    </div>

    <div class='row vspace-top'>
        <div class="col-sm-3 center-align">
                <img src='imgs/2023/ssif-v2_orig.gif' class='img-fluid'>
        </div>

        <div class="col">
            <div class='paper-title'>
                 SSIF: Single-shot Implicit Morphable Faces with Consistent Texture Parameterization
            </div>
            <div class='paper-desc'>
                
            </div>
            <div class='paper-authors'>
               Connor Lin, Koki Nagano, Jan Kautz, Eric R. Chan, <u>Umar Iqbal</u>, Leonidas Guibas, Gordon Wetzstein, Sameh Khamis <br><b>SIGGRAPH 2023</b></u>
            </div>
            <div>
                <a href="https://research.nvidia.com/labs/nxp/lp3d/media/paper.pdf">[PDF]</a>
                <a href="https://drive.google.com/file/d/1NvWxuoXvXjsgn-lZ-jzyUJHqqhf01dKB/view">[Video]</a>
                <a href="https://research.nvidia.com/labs/nxp/lp3d/media/paper.pdf">[Project Page]</a>
            </div>
        </div>
    </div>

    <div class='vspace-top'>
        <h2>2022</h2>
        <hr> 
    </div>


    <div class='row vspace-top'>
        <div class="col-sm-3 center-align">
                <img src='imgs/2022/dracon_1.gif' class='img-fluid'>
        </div>

        <div class="col">
            <div class='paper-title'>
                DRaCoN – Differentiable Rasterization Conditioned Neural Radiance Fields for Articulated Avatars
            </div>
            <div class='paper-desc'>
                
            </div>
            <div class='paper-authors'>
                Amit Raj,<u>Umar Iqbal</u>, Koki Nagano, Sameh Khamis, Pavlo Molchanov, James Hays, Jan Kautz <br><b>arXiv Preprint 2022</b></u>
            </div>
            <div>
                <a href="https://arxiv.org/abs/2203.15798">[PDF]</a>
                <a href="https://arxiv.org/abs/2203.15798">[Project Page]</a>
                
            </div>
        </div>
    </div>

                <div class='row vspace-top'>
                    <div class="col-sm-3 center-align">
                        <img src='imgs/2022/watch-it-move-small.gif' class='img-fluid'>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                            Watch It Move: Unsupervised Discovery of 3D Joints for Re-Posing of Articulated Objects
                        </div>
                        <div class='paper-desc'>
                            
                        </div>
                        <div class='paper-authors'>
                           Atsuhiro Noguchi, <u>Umar Iqbal</u>, Jonathan Tremblay, Tatsuya Harada, Orazio Gallo <br><b>IEEE Conference on Computer Vision and Pattern Recognition (CVPR),  2022</b></u>
                        </div>
                        <div>
                            <a href="https://arxiv.org/abs/2112.11347">[PDF]</a>
                            <a href="https://nvlabs.github.io/watch-it-move/">[Project Page]</a>
                            <a href="https://www.youtube.com/watch?v=oRnnuCVV89o">[Video]</a>
                        </div>
                    </div>
                </div>

                <div class='row vspace-top'>
                    <div class="col-sm-3 center-align">
                        <img src='imgs/2022/glamr-small.gif' class='img-fluid'>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                            GLAMR: Global Occlusion-Aware Human Mesh Recovery with Dynamic Cameras
                        </div>
                        <div class='paper-desc'>
                            
                        </div>
                        <div class='paper-authors'>
                            Ye Yuan, <u>Umar Iqbal</u>, Pavlo Molchanov, Kris Kitani, Jan Kautz <br><b>IEEE Conference on Computer Vision and Pattern Recognition (CVPR),  2022</b></u>
                        </div>
                        <div>
                            <a href="https://arxiv.org/abs/2112.01524">[PDF]</a>
                            <a href=" https://nvlabs.github.io/GLAMR/">[Project Page]</a>
                            <a href="https://www.youtube.com/watch?v=wpObDXcYueo">[Video]</a>
                        
                        </div>
                    </div>
                </div>


                <div class='vspace-top'>
                    <h2>2021</h2>
                    <hr> 
                </div>            
                <div class='row vspace-top'>
                    <div class="col-sm-3 center-align">
                        <img src='imgs/2021/physics-iccv21.gif' class='img-fluid' >
                        
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                            Physics-based Human Motion Estimation and Synthesis from Videos
                        </div>
                        <div class='paper-desc'>
			    
                        </div>
                        <div class='paper-authors'>
                           Kevin Xie, Tingwu Wang, <u>Umar Iqbal</u>, Yunrong Guo, Sanja Fidler, Florian Shkurti <br><b>IEEE Conference on Computer Vision  (ICCV),  2021</b></d>
                        </div>
                        <div>
                            <a href="https://arxiv.org/abs/2109.09913">[PDF]</a>
                            <a href="https://nv-tlabs.github.io/physics-pose-estimation-project-page/">[Project Page]</a>
                            <a href="https://www.youtube.com/watch?v=MrKlPvWvQ2Q">[Video]</a>
                        </div>
                    </div>
                </div>

                <div class='row vspace-top'>
                    <div class="col-sm-3 center-align">
                        <img src='imgs/2021/kama.gif' class='img-fluid'>
        
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                            KAMA: 3D Keypoint Aware Body Mesh Articulation
                        </div>
                        <div class='paper-desc'>
                            
                        </div>
                        <div class='paper-authors'>
                            <u>Umar Iqbal</u>, , Kevin Xie, Yunrong Guo, Jan Kautz, Pavlo Molchanov <br><b>International Conference on 3D Vision (3DV), 2021</b></u>
                        </div>
                        <div>
                            <a href="https://arxiv.org/abs/2104.13502">[PDF]</a>
                            <a href="https://www.youtube.com/watch?v=mPikZEIpUE0">[Qualitative Results]</a>
                        </div>
                    </div>
                </div>

                <div class='row vspace-top'>
                    <div class="col-sm-3 center-align">
                        <div style="width: 173px; height:193px;">
                            <img src='imgs/2021/ssod-iccv21.jpg' class='img-fluid' style="width: 100%; height: 100%;">
                        </div>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
			    Self-Supervised Object Detection via Generative Image Synthesis
                        </div>
                        <div class='paper-desc'>
				
                        </div>
                        <div class='paper-authors'>
                            Siva K. Mustikovela, Shalini De Mello, Aayush Prakash,<u> Umar Iqbal</u>, Sifei Liu, Thu Nguyen-Phuoc, Carsten Rother, Jan Kautz
                            <br><b>IEEE Conference on Computer Vision  (ICCV),  2021</b></u>
                        </div>
                        <div>
                            <a href="https://arxiv.org/abs/2110.09848">[PDF]</a>
                            <a href="https://github.com/NVlabs/SSOD">[Code]</a>
                        </div>
                    </div>
                </div>

                <div class='row vspace-top'>
                    <div class="col-sm-3 center-align">
                        <div style="width: 173px; height:193px;">
                            <img src='imgs/2021/adversarial-hands-2021.jpg' class='img-fluid' style="width: 100%; height: 100%;" >
                        </div>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
		        ​Adversarial Motion Modelling helps Semi-supervised Hand Pose Estimation 
                        </div>
                        <div class='paper-desc'>
				
                        </div>
                        <div class='paper-authors'>
                           Adrian Spurr, Pavlo Molchanov,<u> Umar Iqbal</u>, Jan Kautz, Otmar Hilliges
                           <br><b>arXiv 2021</b>
                        </div>
                        <div>
                            <a href="https://arxiv.org/abs/2106.05954">[PDF]</a>
                            
                        </div>
                    </div>
                </div>

                <div class='row vspace-top'>
                    <div class="col-sm-3 center-align">
                        <div style="width: 173px; height:193px;">
                            <img src='imgs/2021/capture.jpg' class='img-fluid' style="width: 100%; height: 100%;">
                        </div>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                           ​Weakly-Supervised Physically Unconstrained Gaze Estimation
                        </div>
                        <div class='paper-desc'>
			    
                        </div>
                        <div class='paper-authors'>
                           Rakshit Kothari, Shalini De Mello,<u> Umar Iqbal</u>, Wonmin Byeon, Seonwook Park, Jan Kautz
                           <br><b>IEEE Conference on Computer Vision and Pattern Recognition (CVPR),  2021</b></u>
                        </div>
                        <div>
                            <a href="https://arxiv.org/pdf/2105.09803.pdf">[PDF]</a>
                            <a href="https://github.com/NVlabs/weakly-supervised-gaze">[Code]</a>
                        </div>
                    </div>
                </div>


                <div class='row vspace-top'>
                    <div class="col-sm-3 center-align">
                        <div style="width: 173px; height:193px;">
                        <img src='imgs/2021/semitrack-2020.jpg' class='img-fluid' style="width: 100%; height: 100%;">
                    </div>
                    </div>

                    <div class="col">
                      <div class='paper-title'>
                          ​Learning to Track Instances without Video Annotations
                      </div>
                      <div class='paper-desc'>
		      
                      </div>
                      <div class='paper-authors'>
                    Yang Fu, Sifei Liu, <u>Umar Iqbal</u>, Shalini De Mello, Humphrey Shi , Jan Kautz
<br><b>IEEE Conference on Computer Vision and Pattern Recognition (CVPR),  2021</b></u>
                      </div>
                      <div>
                         <a href="https://arxiv.org/abs/2104.00287">[PDF]</a>
                         <a href="https://oasisyang.github.io/projects/semi-track/index.html">[Project Page]</a>
                         
                      </div>
                    </div>
                </div>


                <div class='row vspace-top'>
                    <div class="col-sm-3 center-align">
                        <div style="width: 173px; height:193px;">
                        <img src='imgs/2021/dexycb-cvpr21.jpg' class='img-fluid' style="width: 100%; height: 100%;">
                    </div>
                    </div>

                    <div class="col">
                      <div class='paper-title'>
                         DexYCB: A Benchmark for Capturing Hand Grasping of Objects
                      </div>
                      <div class='paper-desc'>
                        
                      </div>
                      <div class='paper-authors'>
                      Yu-Wei Chao, Wei Yang, Yu Xiang, Pavlo Molchanov, Ankur Handa, Jonathan Tremblay, Yashraj Narang, Karl Van Wyk,
<u>Umar Iqbal</u>, Stan Birchfield, Jan Kautz, Dieter Fox
<br><b>IEEE Conference on Computer Vision and Pattern Recognition (CVPR),  2021</b></u>
                      </div>
                      <div>
                        <a href="https://arxiv.org/abs/2104.04631">[PDF]</a>
                        <a href="https://dex-ycb.github.io/">[Dataset]</a>
                        <a href="https://www.youtube.com/watch?v=Myyv2Uk5MKw">[Video]</a>
                        <a href="https://github.com/NVlabs/dex-ycb-toolkit">[Code]</a>
                      </div>
                    </div>
                </div>
    
    <div class='vspace-top'>
        <h2>2020</h2>
        <hr> 
    </div>
                <div class='row vspace-top'>
                    <div class="col-sm-3 center-align">
                        <div style="width: 173px; height:193px;">
                        <img src='imgs/2020/hand-constraints_1.png' class='img-fluid' style="width: 100%; height: 100%;">
                    </div>
                    </div>

                    <div class="col">
                      <div class='paper-title'>
                          Weakly-Supervised 3D Hand Pose Estimation via Biomechanical Constraints 
                      </div>
                      <div class='paper-desc'>
                          
                      </div>
                      <div class='paper-authors'>
                          Adrian Spurr, <u>Umar Iqbal</u>, Pavlo Molchanov, Otmar Hilliges, Jan Kautz
                           <br><b>European Conference on Computer Vision (ECCV), 2020.</b></u>
                      </div>
                      <div>
                         <a href="https://arxiv.org/pdf/2003.09282.pdf">[PDF]</a>
                         
                      </div>
                    </div>
                </div>

                <div class='row vspace-top'>
                    <div class="col-sm-3 center-align">
                        <div style="width: 173px; height:210px;">
                        <img src='imgs/2020/eccvhandschallenge.jpg' class='img-fluid' style="width: 100%; height: 100%;">
                    </div>
                    </div>

                    <div class="col">
                      <div class='paper-title'>
                       Measuring Generalisation to Unseen Viewpoints, Articulations, Shapes and Objects for 3D Hand Pose Estimation under Hand-Object Interaction 
                      </div>
                      <div class='paper-desc'>
                       
                      </div>
                      <div class='paper-authors'>
                          Anil Armagan, Guillermo Garcia-Hernando, Seungryul Baek, Shreyas Hampali, Mahdi Rad, Zhaohui Zhang, Shipeng Xie, MingXiu Chen,
                           Boshen Zhang, Fu Xiong, Yang Xiao, Zhiguo Cao, Junsong Yuan, Pengfei Ren, Weiting Huang, Haifeng Sun, Marek Hrúz, 
                           Jakub Kanis, Zdeněk Krňoul, Qingfu Wan, Shile Li, Linlin Yang, Dongheui Lee, Angela Yao, Weiguo Zhou, Sijia Mei, Yunhui Liu, Adrian Spurr,
                           <u>Umar Iqbal</u>, Pavlo Molchanov, Philippe Weinzaepfel, Romain Brégier, Gregory Rogez, Vincent Lepetit, Tae-Kyun Kim 
                           <br><b>European Conference on Computer Vision (ECCV), 2020.</b> 
                      </div>
                      <div>
                        <a href="https://arxiv.org/pdf/2003.13764.pdf">[PDF]</a>
                        
                      </div>
                    </div>
                </div>

                <div class='row vspace-top'>
                    <div class="col-sm-3 center-align">
                        <div style="width: 173px; height:193px;">
                        <img src='imgs/2020/multi-view_2.png' class='img-fluid' style="width: 100%; height: 100%;">
                    </div>
                    </div>

                    <div class="col">
                      <div class='paper-title'>
                          Weakly-Supervised 3D Human Pose Learning via Multi-view Images in the Wild
                      </div>
                      <div class='paper-desc'>
                          
                      </div>
                      <div class='paper-authors'>
                          <u>Umar Iqbal</u>, Pavlo Molchanov,  Jan Kautz
<br><b>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Seattle, USA, 2020.</b></u> 
                      </div>
                      <div>
                        <a href="https://arxiv.org/abs/2003.07581">[PDF]</a>
                        <a href="https://arxiv.org/abs/2003.07581">[Video]</a>
                      </div>
                    </div>
                </div>

                <div class='row vspace-top'>
                    <div class="col-sm-3 center-align">
                        <div style="width: 173px; height:193px;">
                            <img src='imgs/2020/selection-173.png' class='img-fluid' style="width: 100%; height: 100%;">
                        </div>
                    </div>
                

                    <div class="col">
                      <div class='paper-title'>
                         ​Self-Supervised Viewpoint Learning from Image Collections
                      </div>
                      <div class='paper-desc'>
                          
                      </div>
                      <div class='paper-authors'>
                          Siva Karthik Mustikovela, Varun Jampani, Shalini De Mello, Sifei Liu, <u>Umar Iqbal</u>, Carsten Rother, Jan Kautz
<br><b>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Seattle, USA, 2020.</b></u> 
                      </div>
                      <div>
                        <a href="https://arxiv.org/pdf/2004.01793.pdf">[PDF]</a>
                        <a href="https://developer.nvidia.com/cvpr/2020/video/cvpr04">[Video]</a>
                        <a href="https://github.com/NVlabs/SSV">[Code]</a>
                      </div>
                    </div>
                </div>

    <div class='vspace-top'>
        <h2>2019 and earlier...</h2>
        <hr> 
    </div>
 

                <div class='row vspace-top'>
                    <div class="col-sm-3 center-align">
                        <div style="width: 173px; height:193px;">
                        <img src='imgs/2019/capture_3.png' class='img-fluid' style="width: 100%; height: 100%;">
                    </div>
                    </div>

                    <div class="col">
                      <div class='paper-title'>
                         Few Shot Adaptive Gaze Estimation
                      </div>
                      <div class='paper-desc'>
                          
                      </div>
                      <div class='paper-authors'>
                          Seonwook Park, Shalini De Mello, Pavlo Molchanov, <u>Umar Iqbal</u>, Otmar Hilliges, Jan Kautz
<br><b>International Conference on Computer Vision (ICCV), Seoul, South Korea, 2019</b></u>
                      </div>
                      <div>
                        <a href="https://arxiv.org/abs/1905.01941">[PDF]</a>
                        
                        <a href="https://github.com/NVlabs/few_shot_gaze">[Code]</a>
                      </div>
                    </div>
                </div>
                <div class='row vspace-top'>
                    <div class="col-sm-3 center-align">
                        <div style="width: 173px; height:193px;">
                        <img src='imgs/2018/thesis.jpg' class='img-fluid' style="width: 100%; height: 100%;">
                    </div>
                    </div>

                    <div class="col">
                      <div class='paper-title'>
                         Articulated Human Pose Estimation in Unconstrained Images and Videos
                      </div>
                      <div class='paper-desc'>
                          Dissertation, University of Bonn, Germany, 2018.
                      </div>
                      <div class='paper-authors'>
                        
                          <span style="color: red; font-weight: bold;">Summa cum laude, DAGM MVTec Dissertation Award 2019</span>
                      </div>
                      <div>
                        <a href="http://hss.ulb.uni-bonn.de/2018/5292/5292.html">[PDF]</a>
                        <a href="http://www.umariqbal.info/uploads/1/4/8/3/14837880/thesis_summary_compressed.pdf">[Summary]</a>
                        <a href="hhttp://www.umariqbal.info/uploads/1/4/8/3/14837880/uiqbal-phd-defense-no-videos.pdf">[Slides]</a>
                        <a href="https://www.youtube.com/watch?v=e8r1usfUays">[Talk]</a>
                      </div>
                    </div>
                </div>
                <div class='row vspace-top'>
                    <div class="col-sm-3 center-align">
                        <div style="width: 173px; height:193px;">
                        <img src='imgs/2018/hand-pose.png' class='img-fluid' style="width: 100%; height: 100%;">
                    </div>
                    </div>

                    <div class="col">
                      <div class='paper-title'>
                         Hand Pose Estimation via Latent 2.5D Heatmap Regression
                      </div>
                      <div class='paper-desc'>
                          
                      </div>
                      <div class='paper-authors'>
                          <u>U. Iqbal</u>, P. Molchanov, T. Breuel, J. Gall and J. Kautz
    <br>
    <b>European Conference on Computer Vision (ECCV'18), Munich, Germany, 2018</b>
    <br>
    Invited to the 4th Workshop on Observing and Understanding Hands in Action (HANDS), 2018

                          <br><span style="color: red; font-weight: bold;">Best Poster Award @ HANDS'18</span>
                      </div>
                      <div>
                        <a href="https://arxiv.org/pdf/1804.09534.pdf">[PDF]</a>
                        <a href="https://www.youtube.com/watch?v=4Q3ByHZ8tNc">[Video]]</a>
                        <a href="http://www.umariqbal.info/uploads/1/4/8/3/14837880/poster-nvidia-style-compressed.pdf">[Poster]</a>
                        <a href="http://www.umariqbal.info/uploads/1/4/8/3/14837880/generate_plots_eccv18.m">[Plots]</a>
                      </div>
                    </div>
                </div>
                <div class='row vspace-top'>
                    <div class="col-sm-3 center-align">
                        <div style="width: 173px; height:193px;">
                        <img src='imgs/2018/jointflow-2018.png' class='img-fluid' style="width: 100%; height: 100%;">
                    </div>
                    </div>

                    <div class="col">
                      <div class='paper-title'>
                         JointFlow: Temporal Flow Fields for Multi Person Pose Tracking
                      </div>
                      <div class='paper-desc'>
                          
                      </div>
                      <div class='paper-authors'>
                        A. Doering,<u> U. Iqbal</u>, J. Gall
<br><b>British Machine Vision Conference (BMVC'18), Newcastle, UK, 2018</b></u>

                         
                      </div>
                      <div>
                        <a href="https://arxiv.org/pdf/1805.04596.pdf">[PDF]</a>
                        
                        <a href="http://www.umariqbal.info/uploads/1/4/8/3/14837880/poster-compressed.pdf">[Poster]</a>
                       
                      </div>
                    </div>
                </div>
                 <div class='row vspace-top'>
                    <div class="col-sm-3 center-align">
                        <div style="width: 173px; height:193px;">
                        <img src='imgs/2018/seq03_1.gif' class='img-fluid' style="width: 100%; height: 100%;">
                    </div>
                    </div>

                    <div class="col">
                      <div class='paper-title'>
                         ​PoseTrack: A Benchmark for Human Pose Estimation and Tracking
                      </div>
                      <div class='paper-desc'>
                          
                      </div>
                      <div class='paper-authors'>
                       M. Andriluka,<u> U. Iqbal</u>, A. Milan, E. Insafutdinov, L. Pishchulin,  J. Gall and B. Schiele
<br><b>IEEE Conference on Computer Vision and Pattern Recognition (CVPR'18),  Salt Lake City, USA, 2018</b></u>

                         
                      </div>
                      <div>
                        <a href="http://www.umariqbal.info/uploads/1/4/8/3/14837880/posetrack_paper.pdf">[PDF]</a>
                        <a href="https://posetrack.net/">[Project Page]</a>
                        <a href="https://www.youtube.com/watch?v=uYFRxGyMDe4">[Video]</a>
                        <a href="https://posetrack.net//users/login.php?dest=users/download.php">[Data]</a>
                        <a href="https://github.com/leonid-pishchulin/poseval">[Code]</a>
                       
                      </div>
                    </div>
                </div>
                 <div class='row vspace-top'>
                    <div class="col-sm-3 center-align">
                        <div style="width: 173px; height:193px;">
                        <img src='imgs/2018/tpami-2017.png' class='img-fluid'style="width: 100%; height: 100%;">
                    </div>
                    </div>

                    <div class="col">
                      <div class='paper-title'>
                         A Dual-Source Approach for 3D Human Pose Estimation from Single Images
                      </div>
                      <div class='paper-desc'>
                          
                      </div>
                      <div class='paper-authors'>
                      <u>U. Iqbal</u>, A. Doering, H. Yasin, B. Krüger, A. Weber, and J. Gall
<br><b>Computer Vision and Image Understanding (CVIU), 2018</b></u>

                         
                      </div>
                      <div>
                        <a href="http://www.umariqbal.info/uploads/1/4/8/3/14837880/cviu2017_3dpose.1.pdf">[PDF]</a>
                        <a href="http://pages.iai.uni-bonn.de/iqbal_umar/ds3dpose/">[Project Page]</a>
                        <a href="https://github.com/iqbalu/3D_Pose_Estimation_CVPR2016.git">[Code]</a>
                       
                      </div>
                    </div>
                </div>
                 <div class='row vspace-top'>
                    <div class="col-sm-3 center-align">
                        <div style="width: 173px; height:193px;">
                        <img src='imgs/2017/posetrack.png' class='img-fluid' style="width: 100%; height: 100%;">
                    </div>
                    </div>

                    <div class="col">
                      <div class='paper-title'>
                        PoseTrack: Joint Multi-Person Pose Estimation and Tracking
                      </div>
                      <div class='paper-desc'>
                          
                      </div>
                      <div class='paper-authors'>
                      <u>U. Iqbal</u>, A. Milan, and J. Gall
<br><b>IEEE Conference on Computer Vision and Pattern Recognition (CVPR'17),  Hawaii, USA, July 2017</b></u>

                         
                      </div>
                      <div>
                        <a href="http://www.umariqbal.info/uploads/1/4/8/3/14837880/posetrack_cvpr17.pdf">[PDF]</a>
                        <a href="http://www.umariqbal.info/uploads/1/4/8/3/14837880/bibtex_actionpose2016.txt">[Bibtex]</a>
                        <a href="http://pages.iai.uni-bonn.de/iqbal_umar/PoseTrack/">[Project Page]</a>
                        <a href="https://www.youtube.com/watch?v=SgiFPWNuAGw">[Video]</a>
                        <a href="http://pages.iai.uni-bonn.de/iqbal_umar/PoseTrack/data/MultiPerson_PoseTrack_v0.1.rar">[Data]</a>
                        <a href="https://github.com/iqbalu/PoseTrack-CVPR2017">[Code]</a>
                       
                      </div>
                    </div>
                </div>
                 <div class='row vspace-top'>
                    <div class="col-sm-3 center-align">
                        <div style="width: 173px; height:193px;">
                        <img src='imgs/2017/golfswing.png' class='img-fluid' style="width: 100%; height: 100%;">
                    </div>
                    </div>

                    <div class="col">
                      <div class='paper-title'>
                        ​Pose for Action - Action for Pose
                      </div>
                      <div class='paper-desc'>
                          
                      </div>
                      <div class='paper-authors'>
                     <u>U. Iqbal</u>, M. Garbade, and J. Gall
<br><b>IEEE Conference on Automatic Face and Gesture Recognition (FG'17), Washington-DC, USA, 2017</b></u>

                         
                      </div>
                      <div>
                        <a href="https://arxiv.org/pdf/1603.04037.pdf">[PDF]</a>
                        <a href="http://www.umariqbal.info/uploads/1/4/8/3/14837880/bibtex_actionpose2016.txt">[Bibtex]</a>
                        <a href="http://pages.iai.uni-bonn.de/iqbal_umar/action4pose/">[Project Page]</a>                     
                        <a href="https://github.com/iqbalu/Action4Pose_FG2017">[Code]</a>
                       
                      </div>
                    </div>
                </div>
                <div class='row vspace-top'>
                    <div class="col-sm-3 center-align">
                        <div style="width: 173px; height:193px;">
                        <img src='imgs/2016/0569_1.png' class='img-fluid' style="width: 100%; height: 100%;">
                    </div>
                    </div>

                    <div class="col">
                      <div class='paper-title'>
                       ​Multi-Person Pose Estimation with Local Joint-to-Person Associations
                      </div>
                      <div class='paper-desc'>
                          
                      </div>
                      <div class='paper-authors'>
                     <u>U. Iqbal</u> and J. Gall
<br><b>Crowd Understanding Workshop (CUW). In conjunction with ECCV'16, Amsterdam, 2016 </b></u>

                         
                      </div>
                      <div>
                        <a href="http://www.umariqbal.info/uploads/1/4/8/3/14837880/paper.pdf">[PDF]</a>
                        <a href="http://www.umariqbal.info/uploads/1/4/8/3/14837880/bibtex.txt">[Bibtex]</a>
                        <a href="http://pages.iai.uni-bonn.de/iqbal_umar/multiperson-pose/">[Project Page]</a>                     
                        <a href="https://github.com/iqbalu/Multi-Person-Pose-Estimation-using-LJPA-ECCVW-2016">[Code]</a>
                       
                      </div>
                    </div>
                </div>
                 <div class='row vspace-top'>
                    <div class="col-sm-3 center-align">
                        <div style="width: 173px; height:193px;">
                        <img src='imgs/2016/dualsource.png' class='img-fluid' style="width: 100%; height: 100%;">
                    </div>
                    </div>

                    <div class="col">
                      <div class='paper-title'>
                       ​​A Dual-Source Approach for 3D Pose Estimation from a Single Image
                      </div>
                      <div class='paper-desc'>
                          
                      </div>
                      <div class='paper-authors'>
                     H. Yasin*,<u> U. Iqbal*</u>, B. Krüger, A. Weber, and J. Gall (*equal contribution)
<br><b>IEEE Conference on Computer Vision and Pattern Recognition (CVPR'16),  Las Vegas, USA, June 2016.</b></u>
 
                         
                      </div>
                      <div>
                        <a href="http://arxiv.org/abs/1509.06720">[PDF]</a>
                        <a href="http://www.umariqbal.info/uploads/1/4/8/3/14837880/bibtex_ds3dpse.txt">[Bibtex]</a>
                        <a href="http://pages.iai.uni-bonn.de/iqbal_umar/ds3dpose/">[Project Page]</a>                     
                        <a href="https://github.com/iqbalu/3D_Pose_Estimation_CVPR2016.git">[Code]</a>
                       
                      </div>
                    </div>
                </div>
                
                <div class='row vspace-top'>
                    <div class="col-sm-3 center-align">
                        <div style="width: 173px; height:193px;">
                        <img src='imgs/2014/selection-002_1.png' class='img-fluid' style="width: 100%; height: 100%;">
                    </div>
                    </div>

                    <div class="col">
                      <div class='paper-title'>
                       ​Who is the Hero? - Semi-Supervised Person Re-Identification in Videos 
                      </div>
                      <div class='paper-desc'>
                          
                      </div>
                      <div class='paper-authors'>
                     <u> U. Iqbal</u> , I. D. D. Curcio, and M. Gabbouj
<br><b>Int. Conference on Computer Vision Theory and Applications (VISAPP'14), Lisbon, Portugal, 2014</b></u>
 
                         
                      </div>
                      <div>
                        <a href="http://www.umariqbal.info/uploads/1/4/8/3/14837880/visapp_2014.pdf">[PDF]</a>
                        <a href="http://www.umariqbal.info/uploads/1/4/8/3/14837880/bibtex_visapp2014.txt">[Bibtex]</a>
                        
                       
                      </div>
                    </div>
                </div>
                <div class='row vspace-top'>
                    <div class="col-sm-3 center-align">
                        <div style="width: 173px; height:193px;">
                        <img src='imgs/2010/1472811373.png' class='img-fluid' style="width: 100%; height: 100%;">
                    </div>
                    </div>

                    <div class="col">
                      <div class='paper-title'>
                       Image Based Vehicle Type Identification
                      </div>
                      <div class='paper-desc'>
                          
                      </div>
                      <div class='paper-authors'>
                     <u> U. Iqbal</u> , S.W. Zamir, M.H. Shahid, K. Parwaiz, M. Yasin and M.S. Sarfraz
<br><b>IEEE International Conference on Information and Emerging Technologies (ICIET'10, Oral),  
​Pakistan, 2010. </b></u>
 
                         
                      </div>
                      <div>
                        <a href="http://www.umariqbal.info/uploads/1/4/8/3/14837880/iqbal2010.pdf">[PDF]</a>
                        <a href="http://www.umariqbal.info/uploads/1/4/8/3/14837880/bibtex_iciet2010.txt">[Bibtex]</a>
                        <a href="http://comvis.ciitlahore.edu.pk/downloads/comvis_cardataset_v1.0.html">[Data]</a>
                       
                      </div>
                    </div>
                </div>
                <br>
                 <div class="project">
                    
      <!--  <h2>Previous Projects</h2>
        <h3>Important person detection from multiple videos (Master's Thesis)</h3>
        <p>The goal of this work is to develop a unified framework for unsupervised person re-identification with application to important person detection from multiple videos. It focuses on topics such as:</p>
        <ul>
            <li><b>Face detection</b></li>
            <li><b>Particle filters based object tracking</b></li>
            <li>Face/clothing <b>recognition</b></li>
            <li>Detection of <b>high level attributes</b> (i.e. Race, Gender, Eyewears, Headwears, Hair colors, Age, etc.). An example can be seen <a href="https://www.youtube.com/watch?v=qkmBCWZ6kTY">here</a>.</li>
            <li>Sensor based <b>temporal segmentation</b> of videos</li>
            <li><b>Supervised classification</b></li>
            <li><b>Hierarchical Agglomerative Clustering (HAC) with constraints</b> (i.e. cannot-link constraints)</li>
        </ul>
        <p><b>Tools used in this work:</b> C++, OpenCV, Python, Intel's Threading Building Blocks (TBB), GNU Scientific Library (GSL)</p>
        <p><b>Link:</b> <a href="http://urn.fi/URN:NBN:fi:tty-201309091321">PDF</a></p>
    </div>
    <div class="project">
        <br>
        <h3>Vehicle make and model recognition (Bachelor's Thesis)</h3>
        <p>
            I worked on this topic as my undergraduate thesis and also followed the same work during my tenure at COMVis. The results of 
             work were published in ICIET 2010 and the paper can be 
             found <a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=5625675&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D5625675">here</a>.
        </p>
        <p>
            Vehicle's database used in this work can be downloaded from <a href="http://comvis.ciitlahore.edu.pk/downloads/comvis_cardataset_v1.0.html">this link</a>.
        </p>
        <p>
            Demo: <span id="demo"></span>
        </p>
    </div>

    <div class="video-container">
        <iframe class="video" width="560" height="315" src="https://www.youtube.com/embed/J1IAkp9Dq5Q" frameborder="0" allowfullscreen></iframe>
    </div>
    <div class="project">
        <br>
        <h3>Preceding Vehicle Detection and Distance Estimation for Driver Assistance System</h3>
        <p>
            
        </p>
        <p>
            Technical Report on the project can be found here. (Last updated: 31-12-2010)
        </p>
        <p>
            Demo: <span id="demo"></span>
        </p>
    </div>

    <div class="video-container">
        <iframe class="video" width="560" height="315" src="https://www.youtube.com/embed/sCbcQso1SVE" frameborder="0" allowfullscreen></iframe>
    </div>-->
           

                
                
              
                
    </div><!-- /container (news + publications) -->


    <div id='footer' class='vspace-top'>
        <p style="margin:0;">© Umar Iqbal &nbsp;·&nbsp; <a href="mailto:uiqbal@nvidia.com">uiqbal@nvidia.com</a></p>
    </div>

    <script>
    document.addEventListener('DOMContentLoaded', () => {

        // ── Scroll-reveal fade-in ──────────────────
        const revealObserver = new IntersectionObserver((entries) => {
            entries.forEach(entry => {
                if (entry.isIntersecting) {
                    entry.target.classList.add('visible');
                    revealObserver.unobserve(entry.target);
                }
            });
        }, { threshold: 0.08, rootMargin: '0px 0px -24px 0px' });

        // Apply fade-in to paper rows and year headings (skip items already in viewport)
        function applyFadeIn(el) {
            const rect = el.getBoundingClientRect();
            if (rect.top < window.innerHeight && rect.bottom > 0) {
                // Already visible — mark as seen immediately, no transition
                el.classList.add('fade-in', 'visible');
            } else {
                el.classList.add('fade-in');
                revealObserver.observe(el);
            }
        }
        document.querySelectorAll('.col-sm-9.paper-col').forEach(el => {
            const row = el.closest('.row');
            if (row) applyFadeIn(row);
        });
        document.querySelectorAll('.vspace-top:has(h2)').forEach(el => {
            applyFadeIn(el);
        });

        // ── Active nav link on scroll ──────────────
        const sectionIds = ['about', 'news', 'publications'];
        const navLinks = document.querySelectorAll('.nav-links a');

        const sectionObserver = new IntersectionObserver((entries) => {
            entries.forEach(entry => {
                if (entry.isIntersecting) {
                    navLinks.forEach(link => {
                        link.classList.toggle('active',
                            link.getAttribute('href') === '#' + entry.target.id);
                    });
                }
            });
        }, { threshold: 0.2 });

        sectionIds.forEach(id => {
            const el = document.getElementById(id);
            if (el) sectionObserver.observe(el);
        });
    });
    </script>
</body>

</html>